import chainlit as cl
import json
import networkx as nx
from networkx.readwrite import json_graph
import os
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime
import asyncio
import re

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OpenAI import with compatibility handling
try:
    from openai import OpenAI
    OPENAI_V1 = True
    logger.info("Using OpenAI v1+ API")
except ImportError:
    try:
        import openai
        OPENAI_V1 = False
        logger.info("Using OpenAI legacy API")
    except ImportError:
        raise ImportError("OpenAI library not found. Please install: pip install openai")

class CompatibleKnowledgeGraphChatbot:
    """Knowledge graph chatbot with OpenAI compatibility handling"""
    
    def __init__(self, kg_path: str = "knowledge_graph.json", model: str = "gpt-4o"):
        self.kg_path = kg_path
        self.model = model
        self.graph = None
        self.client = None
        self.entity_cache = {}
        self.frequent_queries = {
            "èª²ç¨‹è¦æ±‚": ["å­¸å£«å­¸ä½èª²ç¨‹", "ä¿®è®€å¹´æœŸåŠç•¢æ¥­å­¸åˆ†", "ç•¢æ¥­è³‡æ ¼"],
            "å­¸ç”Ÿæ”¿ç­–": ["å­¸ç”Ÿç´€å¾‹å®ˆå‰‡", "å­¸ç”Ÿ", "å¹³ç­‰æ©Ÿæœƒæ”¿ç­–"],
            "è€ƒè©¦è¦å®š": ["è€ƒè©¦", "è©•ä¼°æˆ–è€ƒæ ¸", "è£œè€ƒ"],
            "ç ”ç©¶å€«ç†": ["ç ”ç©¶é“å¾·æº–å‰‡", "ç ”ç©¶è€…", "ç ”ç©¶åƒèˆ‡è€…"],
            "AIå·¥å…·": ["ç”Ÿæˆå¼ AI å·¥å…·", "å­¸æ¥­ä¸èª å¯¦è¡Œç‚º"],
            "å¯¦ç¿’ç›¸é—œ": ["å¯¦ç¿’", "å¯¦ç¿’å–®ä½", "æœ‰é—œå­¸ç”Ÿå¯¦ç¿’æ”¯æ´"]
        }
        self.load_knowledge_graph()
        self.setup_openai()
    
    def setup_openai(self):
        """Initialize OpenAI client with compatibility handling"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
        
        try:
            if OPENAI_V1:
                # Modern OpenAI v1+ API
                self.client = OpenAI(api_key=api_key)
                logger.info(f"âœ… OpenAI v1+ client initialized with model: {self.model}")
            else:
                # Legacy OpenAI API
                openai.api_key = api_key
                self.client = openai
                logger.info(f"âœ… Legacy OpenAI client initialized with model: {self.model}")
                
        except Exception as e:
            logger.error(f"âŒ Failed to initialize OpenAI client: {e}")
            raise ValueError(f"Cannot initialize OpenAI client: {e}")
    
    def load_knowledge_graph(self):
        """Load knowledge graph from JSON file"""
        try:
            with open(self.kg_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.graph = json_graph.node_link_graph(data)
            logger.info(f"âœ… Knowledge graph loaded: {len(self.graph.nodes)} nodes, {len(self.graph.edges)} edges")
            
            # Store metadata
            self.metadata = data.get('metadata', {})
            
            # Build entity cache for faster searching
            self._build_entity_cache()
            
        except FileNotFoundError:
            logger.error(f"âŒ Knowledge graph file not found: {self.kg_path}")
            raise
        except Exception as e:
            logger.error(f"âŒ Error loading knowledge graph: {e}")
            raise
    
    def _build_entity_cache(self):
        """Build cache for faster entity searching"""
        self.entity_cache = {
            'by_keywords': {},
            'entity_info': {}
        }
        
        for node in self.graph.nodes(data=True):
            entity_name = node[0]
            entity_data = node[1]
            
            # Store entity information
            self.entity_cache['entity_info'][entity_name] = {
                'name': entity_name,
                'type': entity_data.get('type', 'entity'),
                'size': entity_data.get('size', 5),
                'connections': len(list(self.graph.neighbors(entity_name)))
            }
    
    def search_entities(self, query: str, max_results: int = 10) -> List[Dict]:
        """Enhanced entity search with scoring"""
        query_lower = query.lower()
        matches = []
        
        for entity in self.graph.nodes():
            score = 0
            
            # Exact match
            if query_lower == entity.lower():
                score = 100
            # Contains query
            elif query_lower in entity.lower():
                score = 80
            # Character overlap for Chinese
            elif self._calculate_character_overlap(query, entity) > 0.5:
                score = 60
            # Keyword matches
            elif any(keyword in entity.lower() for keyword in query_lower.split()):
                score = 40
            
            if score > 0:
                matches.append({
                    'entity': entity,
                    'score': score,
                    'info': self.entity_cache['entity_info'].get(entity, {})
                })
        
        # Sort by score and return top results
        matches.sort(key=lambda x: x['score'], reverse=True)
        return matches[:max_results]
    
    def _calculate_character_overlap(self, query: str, entity: str) -> float:
        """Calculate character overlap ratio for Chinese text"""
        query_chars = set(query)
        entity_chars = set(entity)
        
        if not query_chars:
            return 0
        
        overlap = len(query_chars.intersection(entity_chars))
        return overlap / len(query_chars)
    
    def get_entity_relationships(self, entity: str) -> Dict:
        """Get relationships for a specific entity"""
        if entity not in self.graph:
            return {}
        
        relationships = {
            'outgoing': [],  # entity -> other
            'incoming': [],  # other -> entity
        }
        
        # Outgoing relationships
        for target in self.graph.successors(entity):
            edge_data = self.graph.get_edge_data(entity, target)
            for edge_attrs in edge_data.values():
                relationships['outgoing'].append({
                    'target': target,
                    'relation': edge_attrs.get('relation', 'related')
                })
        
        # Incoming relationships
        for source in self.graph.predecessors(entity):
            edge_data = self.graph.get_edge_data(source, entity)
            for edge_attrs in edge_data.values():
                relationships['incoming'].append({
                    'source': source,
                    'relation': edge_attrs.get('relation', 'related')
                })
        
        return relationships
    
    def get_context_for_query(self, query: str) -> str:
        """Get relevant context from knowledge graph"""
        # Search for relevant entities
        entities = self.search_entities(query, 5)
        
        if not entities:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„çŸ¥è­˜åœ–è­œä¿¡æ¯ã€‚"
        
        context_parts = []
        context_parts.append("ç›¸é—œå¯¦é«”å’Œé—œä¿‚:")
        
        for entity_info in entities:
            entity = entity_info['entity']
            score = entity_info['score']
            
            context_parts.append(f"\nã€{entity}ã€‘(ç›¸é—œåº¦: {score})")
            
            # Get relationships
            relationships = self.get_entity_relationships(entity)
            
            # Add outgoing relationships
            if relationships['outgoing']:
                context_parts.append("é—œä¿‚:")
                for rel in relationships['outgoing'][:5]:
                    context_parts.append(f"  â€¢ {entity} {rel['relation']} {rel['target']}")
            
            # Add incoming relationships
            if relationships['incoming']:
                context_parts.append("è¢«é—œè¯:")
                for rel in relationships['incoming'][:3]:
                    context_parts.append(f"  â€¢ {rel['source']} {rel['relation']} {entity}")
        
        return "\n".join(context_parts)
    
    def call_openai_api(self, messages: List[Dict], temperature: float = 0.2, max_tokens: int = 1500) -> str:
        """Call OpenAI API with compatibility handling"""
        try:
            if OPENAI_V1:
                # Modern API
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content
            else:
                # Legacy API
                response = self.client.ChatCompletion.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content
                
        except Exception as e:
            logger.error(f"âŒ OpenAI API error: {e}")
            return f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚å‡ºç¾éŒ¯èª¤: {str(e)}"
    
    def generate_response(self, user_query: str) -> Dict:
        """Generate response using knowledge graph and GPT"""
        
        # Get context from knowledge graph
        kg_context = self.get_context_for_query(user_query)
        
        # Find relevant entities for metadata
        entities = self.search_entities(user_query, 5)
        
        # Prepare GPT prompt
        system_prompt = """ä½ æ˜¯æ¾³é–€æ—…éŠå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå°ˆé–€å›ç­”é—œæ–¼å¤§å­¸æ”¿ç­–ã€èª²ç¨‹å’Œè¦å®šçš„å•é¡Œã€‚

è«‹æ ¹æ“šæä¾›çš„çŸ¥è­˜åœ–è­œä¿¡æ¯å›ç­”ç”¨æˆ¶å•é¡Œã€‚å›ç­”è¦æ±‚ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. åŸºæ–¼çŸ¥è­˜åœ–è­œæä¾›æº–ç¢ºä¿¡æ¯
3. çµæ§‹æ¸…æ™°ï¼Œæ˜“æ–¼ç†è§£
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹èª å¯¦èªªæ˜ä¸¦å»ºè­°è¯ç¹«ç›¸é—œéƒ¨é–€
5. ä¿æŒå°ˆæ¥­å’Œå‹å¥½çš„èªèª¿"""

        user_prompt = f"""ç”¨æˆ¶å•é¡Œ: {user_query}

çŸ¥è­˜åœ–è­œç›¸é—œä¿¡æ¯:
{kg_context}

è«‹åŸºæ–¼ä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚"""

        # Generate response
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        answer = self.call_openai_api(messages)
        
        return {
            'answer': answer,
            'relevant_entities': [e['entity'] for e in entities[:5]],
            'context_used': len(entities) > 0
        }

# Initialize the chatbot
kg_chatbot = None

@cl.on_chat_start
async def start():
    """Initialize the chatbot when chat starts"""
    global kg_chatbot
    
    try:
        # Show loading message
        loading_msg = cl.Message(content="ğŸ”„ æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åœ–è­œç³»çµ±...")
        await loading_msg.send()
        
        # Initialize the knowledge graph chatbot
        kg_chatbot = CompatibleKnowledgeGraphChatbot()
        
        # Update loading message
        await loading_msg.update(content="âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        
        # Welcome message
        welcome_msg = f"""# ğŸ“ æ¾³é–€æ—…éŠå¤§å­¸æ™ºèƒ½åŠ©æ‰‹

æ­¡è¿ä½¿ç”¨æ¾³é–€æ—…éŠå¤§å­¸æ™ºèƒ½çŸ¥è­˜åœ–è­œåŠ©æ‰‹ï¼

## ğŸ“Š çŸ¥è­˜åº«çµ±è¨ˆ
- ğŸ§  **å¯¦é«”æ•¸é‡**: {len(kg_chatbot.graph.nodes)} å€‹
- ğŸ”— **é—œä¿‚æ•¸é‡**: {len(kg_chatbot.graph.edges)} å€‹  
- ğŸ“… **æœ€å¾Œæ›´æ–°**: {kg_chatbot.metadata.get('created', 'N/A')}
- ğŸ¤– **AIæ¨¡å‹**: {kg_chatbot.model}

## ğŸ” æˆ‘å¯ä»¥å¹«åŠ©æ‚¨æŸ¥è©¢

### ğŸ“š å­¸è¡“ç›¸é—œ
- èª²ç¨‹è¦æ±‚å’Œç•¢æ¥­æ¢ä»¶
- è©•ä¼°æ–¹æ³•å’Œè€ƒè©¦è¦å®š
- å­¸åˆ†è¨ˆç®—å’Œè½‰æ›

### ğŸ‘¨â€ğŸ“ å­¸ç”Ÿäº‹å‹™  
- å­¸ç”Ÿæ¬Šåˆ©å’Œè²¬ä»»
- ç´€å¾‹å®ˆå‰‡å’Œè™•åˆ†ç¨‹åº
- ç”³è«‹æµç¨‹å’Œæ‰‹çºŒ

### ğŸ”¬ ç ”ç©¶æ”¯æ´
- ç ”ç©¶é“å¾·æº–å‰‡
- å­¸è¡“èª ä¿¡æ”¿ç­–
- å¯¦ç¿’å®‰æ’å’Œæ”¯æ´

### ğŸ¤– AIå·¥å…·ä½¿ç”¨
- ç”Ÿæˆå¼AIå·¥å…·æ”¿ç­–
- å­¸è¡“èª å¯¦è¡Œç‚ºè¦ç¯„

è«‹è¼¸å…¥æ‚¨çš„å•é¡Œé–‹å§‹æŸ¥è©¢ï¼"""

        await cl.Message(content=welcome_msg).send()
        
        # Add quick action buttons
        actions = [
            cl.Action(name="èª²ç¨‹è¦æ±‚", value="èª²ç¨‹è¦æ±‚", description="æŸ¥è©¢èª²ç¨‹å’Œç•¢æ¥­è¦æ±‚"),
            cl.Action(name="å­¸ç”Ÿæ”¿ç­–", value="å­¸ç”Ÿæ”¿ç­–", description="äº†è§£å­¸ç”Ÿç›¸é—œæ”¿ç­–"),
            cl.Action(name="è€ƒè©¦è¦å®š", value="è€ƒè©¦è¦å®š", description="æŸ¥çœ‹è€ƒè©¦å’Œè©•ä¼°è¦å®š"),
            cl.Action(name="ç ”ç©¶å€«ç†", value="ç ”ç©¶å€«ç†", description="ç ”ç©¶é“å¾·å’Œå­¸è¡“èª ä¿¡"),
            cl.Action(name="AIå·¥å…·", value="AIå·¥å…·", description="ç”Ÿæˆå¼AIå·¥å…·ä½¿ç”¨æŒ‡å¼•"),
        ]
        
        await cl.Message(
            content="ğŸš€ **å¿«é€ŸæŸ¥è©¢**: é»æ“Šä¸‹æ–¹æŒ‰éˆ•å¿«é€ŸæŸ¥è©¢å¸¸è¦‹å•é¡Œ",
            actions=actions
        ).send()
        
    except Exception as e:
        error_msg = f"""âŒ **åˆå§‹åŒ–éŒ¯èª¤**

éŒ¯èª¤ä¿¡æ¯: {str(e)}

è«‹æª¢æŸ¥ï¼š
1. `knowledge_graph.json` æ–‡ä»¶æ˜¯å¦å­˜åœ¨
2. OpenAI API å¯†é‘°æ˜¯å¦æ­£ç¢ºè¨­å®š (export OPENAI_API_KEY=sk-...)
3. OpenAI åº«ç‰ˆæœ¬ (å˜—è©¦: pip install openai>=1.0.0)
4. ç¶²çµ¡é€£æ¥æ˜¯å¦æ­£å¸¸

**æ•…éšœæ’é™¤å»ºè­°:**
```bash
# æª¢æŸ¥ API å¯†é‘°
echo $OPENAI_API_KEY

# æ›´æ–° OpenAI åº«
pip install --upgrade openai

# æª¢æŸ¥æ–‡ä»¶
ls -la knowledge_graph.json
```

æ‚¨å¯ä»¥å˜—è©¦é‡æ–°é–‹å§‹å°è©±ã€‚"""
        await cl.Message(content=error_msg).send()

@cl.action_callback("èª²ç¨‹è¦æ±‚")
async def query_course_requirements(action):
    """Handle course requirements query"""
    await process_query("è«‹å‘Šè¨´æˆ‘é—œæ–¼å­¸å£«å­¸ä½èª²ç¨‹çš„è¦æ±‚å’Œç•¢æ¥­æ¢ä»¶")

@cl.action_callback("å­¸ç”Ÿæ”¿ç­–")
async def query_student_policies(action):
    """Handle student policies query"""
    await process_query("å­¸ç”Ÿç´€å¾‹å®ˆå‰‡å’Œç›¸é—œæ”¿ç­–æœ‰å“ªäº›ï¼Ÿ")

@cl.action_callback("è€ƒè©¦è¦å®š")
async def query_exam_regulations(action):
    """Handle exam regulations query"""
    await process_query("è€ƒè©¦å’Œè©•ä¼°çš„ç›¸é—œè¦å®šæ˜¯ä»€éº¼ï¼Ÿ")

@cl.action_callback("ç ”ç©¶å€«ç†")
async def query_research_ethics(action):
    """Handle research ethics query"""
    await process_query("ç ”ç©¶é“å¾·æº–å‰‡å’Œå­¸è¡“èª ä¿¡æ”¿ç­–")

@cl.action_callback("AIå·¥å…·")
async def query_ai_tools(action):
    """Handle AI tools query"""
    await process_query("ç”Ÿæˆå¼AIå·¥å…·çš„ä½¿ç”¨æ”¿ç­–å’ŒæŒ‡å¼•")

@cl.on_message
async def main(message: cl.Message):
    """Handle user messages"""
    await process_query(message.content)

async def process_query(user_query: str):
    """Process user query"""
    global kg_chatbot
    
    if not kg_chatbot:
        await cl.Message(content="âŒ ç³»çµ±æœªåˆå§‹åŒ–ï¼Œè«‹é‡æ–°é–‹å§‹å°è©±ã€‚").send()
        return
    
    # Show processing steps
    async with cl.Step(name="ğŸ” æœç´¢çŸ¥è­˜åœ–è­œ", type="tool") as search_step:
        search_step.output = "æ­£åœ¨æœç´¢ç›¸é—œå¯¦é«”..."
        
        # Search for relevant entities
        entities = kg_chatbot.search_entities(user_query, 5)
        
        if entities:
            entity_list = [f"{e['entity']} (ç›¸é—œåº¦: {e['score']})" for e in entities]
            search_step.output = f"âœ… æ‰¾åˆ° {len(entities)} å€‹ç›¸é—œå¯¦é«”:\n" + "\n".join([f"â€¢ {e}" for e in entity_list])
        else:
            search_step.output = "âš ï¸ æœªæ‰¾åˆ°ç›´æ¥åŒ¹é…çš„å¯¦é«”ï¼Œå°‡é€²è¡Œæ™ºèƒ½åˆ†æ..."
    
    async with cl.Step(name="ğŸ¤– ç”Ÿæˆå›ç­”", type="llm") as response_step:
        response_step.output = "æ­£åœ¨ä½¿ç”¨ AI ç”Ÿæˆæ™ºèƒ½å›ç­”..."
        
        # Generate response
        result = kg_chatbot.generate_response(user_query)
        
        response_step.output = "âœ… å›ç­”å·²ç”Ÿæˆ"
    
    # Send main response
    response_content = result['answer']
    
    # Add entity information if available
    if result['relevant_entities']:
        response_content += f"\n\n## ğŸ“‹ ç›¸é—œå¯¦é«”\n"
        for entity in result['relevant_entities']:
            response_content += f"â€¢ {entity}\n"
    
    await cl.Message(content=response_content).send()

@cl.on_stop
async def stop():
    """Clean up when chat stops"""
    goodbye_msg = """ğŸ‘‹ **æ„Ÿè¬ä½¿ç”¨æ¾³é–€æ—…éŠå¤§å­¸æ™ºèƒ½åŠ©æ‰‹ï¼**

å¸Œæœ›æˆ‘çš„å›ç­”å°æ‚¨æœ‰æ‰€å¹«åŠ©ã€‚å¦‚éœ€æ›´å¤šä¿¡æ¯ï¼Œè«‹ï¼š

ğŸ“ **è¯ç¹«æ•™å‹™éƒ¨**: æŸ¥è©¢å­¸è¡“ç›¸é—œå•é¡Œ
ğŸ« **è¨ªå•å®˜æ–¹ç¶²ç«™**: ç²å–æœ€æ–°æ”¿ç­–æ›´æ–°  
ğŸ“§ **ç™¼é€éƒµä»¶**: ç²å–å…·é«”æ¡ˆä¾‹å”åŠ©

ç¥æ‚¨å­¸ç¿’é †åˆ©ï¼ğŸ“"""
    
    await cl.Message(content=goodbye_msg).send()

if __name__ == "__main__":
    print("ğŸš€ Starting Compatible Chainlit Knowledge Graph Chatbot...")
    print("ğŸ”§ OpenAI Compatibility: Automatic detection")
    print("ğŸ“‹ Run with: chainlit run fixed_chainlit_chatbot.py -w")
    print("ğŸŒ Then open: http://localhost:8000")
